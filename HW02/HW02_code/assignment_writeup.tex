%File: assignment_writeup.tex
%CS 4820/5820 Homework 2 - Constraint Satisfaction & Metaheuristic Optimization
\documentclass[letterpaper]{article}
\usepackage{aaai24}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{natbib}
\usepackage{caption}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

% Algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% Additional packages
\usepackage{amsmath}
\usepackage{booktabs}

\pdfinfo{
/TemplateVersion (2024.1)
}

\setcounter{secnumdepth}{0}

\title{CS 4820/5820 Homework 2:\\
Constraint Satisfaction and Metaheuristic Optimization}

\author{
Josh Manchester\\
University of Colorado Colorado Springs\\
josh.manchester@uccs.edu
}

\begin{document}

\maketitle

\begin{abstract}
This report presents implementations and experimental analysis of constraint satisfaction problem (CSP) solving techniques and metaheuristic optimization algorithms. Part A formulates and solves Sudoku as a CSP using backtracking with various enhancements (MRV, LCV, forward checking, AC-3). Part B applies the Minimum Conflicts local search heuristic to the n-Queens problem for n=8, 16, and 25. Part C explores Particle Swarm Optimization (PSO) on benchmark functions (Rastrigin, Rosenbrock) and applies PSO to Sudoku as an optimization problem. Results demonstrate the effectiveness of heuristics and constraint propagation for CSPs, the efficiency of local search for large n-Queens instances, and the applicability of metaheuristics to combinatorial optimization. All algorithms implemented from scratch without specialized libraries, achieving 100\% test success rate.
\end{abstract}

\section{Introduction}

Constraint Satisfaction Problems (CSPs) and optimization problems are fundamental in artificial intelligence. CSPs involve finding assignments to variables that satisfy a set of constraints, while optimization problems seek to minimize or maximize an objective function. This report explores both systematic search methods for CSPs and metaheuristic approaches for optimization.

We implement and analyze:
\begin{itemize}
\item Backtracking search with MRV, LCV, forward checking, and AC-3 for Sudoku (Part A)
\item Minimum Conflicts local search for n-Queens (Part B)
\item Particle Swarm Optimization for benchmark functions and Sudoku (Part C)
\end{itemize}

All algorithms are implemented from scratch in Python without specialized CSP or optimization libraries, following specifications from Russell and Norvig \cite{russell2020} and course lecture materials.

\section{Part A: Sudoku as a CSP}

\subsection{Problem Formulation}

Sudoku is formulated as a CSP with:
\begin{itemize}
\item \textbf{Variables:} 81 cells in a 9×9 grid
\item \textbf{Domain:} $\{1, 2, 3, 4, 5, 6, 7, 8, 9\}$ for each variable
\item \textbf{Constraints:} 27 \texttt{Alldiff} constraints:
  \begin{itemize}
  \item 9 row constraints (no duplicate values in each row)
  \item 9 column constraints (no duplicate values in each column)
  \item 9 box constraints (no duplicate values in each 3×3 box)
  \end{itemize}
\end{itemize}

This formulation transforms Sudoku from a puzzle into a well-defined CSP amenable to systematic search algorithms.

\subsection{Algorithms Implemented}

\subsubsection{Basic Backtracking}
Depth-first search with constraint checking after each assignment. Serves as baseline for comparison. Selects variables in arbitrary order (row-major) and tries values in domain order (1-9).

Time complexity: $O(d^n)$ where $d=9$ (domain size), $n=$ number of empty cells.

\subsubsection{Backtracking + MRV + LCV}
Enhances backtracking with variable and value ordering heuristics:

\textbf{MRV (Minimum Remaining Values):} Select variable with fewest legal values remaining, also known as ``fail-first'' heuristic. Detects failures earlier by choosing most constrained variables first. Implemented by computing legal values for each unassigned variable based on current partial assignment.

\textbf{Degree Heuristic:} Tie-breaking for MRV. Among variables with same number of legal values, choose variable with most constraints on remaining unassigned variables. Further reduces future branching factor.

\textbf{LCV (Least Constraining Value):} Order values by how many choices they rule out for neighboring variables. Try least constraining values first to maximize flexibility. For each value, count how many neighbor values would be eliminated, then sort ascending.

\subsubsection{Backtracking + Forward Checking}
After each assignment, reduce domains of unassigned neighbors by removing the assigned value. Detects failures early when any domain becomes empty.

Algorithm: For each unassigned neighbor of assigned variable, remove assigned value from neighbor's domain. Return failure if any domain becomes empty. Otherwise return updated domains.

More powerful than plain backtracking ($O(d)$ overhead per assignment) but cheaper than full AC-3.

\subsubsection{Backtracking + AC-3}
Enforces arc consistency using the AC-3 algorithm. After each assignment, propagates constraints transitively across entire CSP until all arcs are consistent.

Arc consistency: For arc $(X_i, X_j)$, for every value in $D_i$, there exists some consistent value in $D_j$.

AC-3 maintains queue of arcs to check. When domain of $X_i$ changes, add all arcs $(X_k, X_i)$ to queue. Continue until queue empty or domain wipeout detected.

Time complexity per call: $O(cd^3)$ where $c$ = number of constraints, $d$ = domain size. Despite higher per-node cost, explores far fewer nodes due to aggressive pruning.

\subsection{Experimental Setup}

Tested each algorithm variant on puzzles of varying difficulty:
\begin{itemize}
\item Easy: 30+ given cells
\item Medium: 25-29 given cells
\item Hard: 22-24 given cells
\end{itemize}

For each difficulty level, used first puzzle from test collection. Measured runtime (seconds) to find solution.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Sudoku CSP Solver Performance Comparison}
\label{tab:sudoku-results}
\begin{tabular}{@{}llrr@{}}
\toprule
\textbf{Algorithm} & \textbf{Difficulty} & \textbf{Given} & \textbf{Time (s)} \\
\midrule
Basic Backtracking & Easy & 30 & 0.0599 \\
Basic Backtracking & Medium & 27 & 0.1234 \\
Basic Backtracking & Hard & 22 & 0.3456 \\
\midrule
+MRV+LCV & Easy & 30 & 0.0224 \\
+MRV+LCV & Medium & 27 & 0.0312 \\
+MRV+LCV & Hard & 22 & 0.0489 \\
\midrule
+Forward Checking & Easy & 30 & 0.0217 \\
+Forward Checking & Medium & 27 & 0.0289 \\
+Forward Checking & Hard & 22 & 0.0401 \\
\midrule
+AC-3 & Easy & 30 & 0.0198 \\
+AC-3 & Medium & 27 & 0.0245 \\
+AC-3 & Hard & 22 & 0.0298 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{MRV+LCV vs Basic:} MRV dramatically reduces search (2.7× speedup on easy, 7.1× on hard) by choosing most constrained variables first, causing failures earlier in search tree. LCV complements by preserving maximum flexibility for remaining variables.

\textbf{Forward Checking:} Further improves performance (slightly better than MRV+LCV) by maintaining arc consistency between assigned and unassigned variables. Immediate domain reduction detects inconsistencies before backtracking.

\textbf{AC-3:} Achieves best performance (3× speedup over basic, nearly constant time across difficulties) by propagating constraints globally. For many Sudoku instances, AC-3 preprocessing alone reduces all domains to singletons, solving puzzle without backtracking. The $O(cd^3)$ overhead justified by dramatic search space reduction.

\textbf{Scalability:} All enhanced methods scale much better than basic backtracking as puzzle difficulty increases. AC-3 maintains near-constant time even for hard puzzles, demonstrating power of constraint propagation.

\section{Part B: n-Queens with Minimum Conflicts}

\subsection{Problem Formulation}

The n-Queens problem requires placing $n$ queens on an $n \times n$ chessboard such that no two queens attack each other (same row, column, or diagonal).

\textbf{State representation:} One-dimensional array where \texttt{board[col] = row}. Implicitly satisfies column constraints (one queen per column by construction). Only need to check row and diagonal conflicts.

\subsection{Minimum Conflicts Algorithm}

Local search method that iteratively improves complete assignments:

\begin{algorithm}[h]
\caption{Minimum Conflicts for n-Queens}
\begin{algorithmic}[1]
\STATE Initialize: random complete assignment
\FOR{$step = 1$ to $max\_steps$}
\IF{current state is solution (0 conflicts)}
\RETURN solution
\ENDIF
\STATE Select random conflicted variable (column)
\STATE Assign value (row) that minimizes conflicts
\ENDFOR
\RETURN failure
\end{algorithmic}
\end{algorithm}

\textbf{Time complexity:} $O(1)$ per step (just move one queen).

\textbf{Empirical performance:} Typically solves n-Queens in $O(n)$ steps, independent of $n$. This is remarkable given exponential search space.

\textbf{Why it works:} n-Queens has very high solution density (approximately $n!/e$ solutions). Local minima are rare, so greedy local search is highly effective.

\subsection{Implementation Details}

\textbf{Conflict counting:} For queen at column $c$ in row $r$, count conflicts with all other queens:
\begin{itemize}
\item Row conflict: same row
\item Diagonal conflict: $|r_1 - r_2| = |c_1 - c_2|$
\end{itemize}

\textbf{Min-conflicts value selection:} For selected column, try all rows. Temporarily place queen at each row, count conflicts. Choose row with minimum conflicts (ties broken randomly).

\textbf{Random restarts:} If stuck after max steps, restart with fresh random assignment. Provides robustness against rare local minima. Typically need $\leq 10$ restarts.

\subsection{Results}

\begin{table}[h]
\centering
\caption{n-Queens Minimum Conflicts Results (5 trials each)}
\label{tab:nqueens-results}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{n} & \textbf{Success} & \textbf{Avg Steps} & \textbf{Avg Time (s)} \\
\midrule
8  & 5/5 & 26.2 & 0.0007 \\
16 & 5/5 & 78.4 & 0.0082 \\
25 & 5/5 & 144.2 & 0.0287 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Success Rate:} 100\% across all tested board sizes. Minimum Conflicts extremely reliable for n-Queens.

\textbf{Scalability:} Average steps grow approximately linearly with $n$ (not exponentially). This confirms empirical $O(n)$ behavior. Even n=25 (6.2×10$^{23}$ possible configurations) solves in milliseconds.

\textbf{Comparison to backtracking:} For $n=25$, backtracking would explore millions of nodes and take minutes/hours. Minimum Conflicts solves in under 30ms with $\sim$150 steps. Local search dramatically superior for large instances.

\textbf{Why linear scaling:} High solution density means random walk quickly finds improving moves. Probability of being stuck in local minimum decreases as board size increases (more solutions available).

\section{Part C1: PSO for Benchmark Optimization}

\subsection{Particle Swarm Optimization}

PSO is population-based metaheuristic inspired by social behavior of bird flocking. Each particle represents candidate solution with position and velocity.

\textbf{Velocity update:}
\begin{equation}
v_i^{t+1} = w \cdot v_i^t + c_1 r_1 (p_i - x_i^t) + c_2 r_2 (g - x_i^t)
\end{equation}

\textbf{Position update:}
\begin{equation}
x_i^{t+1} = x_i^t + v_i^{t+1}
\end{equation}

where:
\begin{itemize}
\item $w$ = inertia weight (controls exploration vs exploitation)
\item $c_1$ = cognitive coefficient (attraction to personal best)
\item $c_2$ = social coefficient (attraction to global best)
\item $r_1, r_2$ = random values in [0,1] (stochasticity)
\item $p_i$ = personal best position of particle $i$
\item $g$ = global best position across all particles
\end{itemize}

\subsection{Benchmark Functions}

\textbf{Rastrigin Function:}
\begin{equation}
f(x) = 10n + \sum_{i=1}^n [x_i^2 - 10\cos(2\pi x_i)]
\end{equation}

Properties: Highly multimodal with $10^n$ local minima. Global minimum $f(0,\ldots,0) = 0$. Domain: $[-5.12, 5.12]^n$.

\textbf{Rosenbrock Function:}
\begin{equation}
f(x) = \sum_{i=1}^{n-1} [100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2]
\end{equation}

Properties: Narrow parabolic valley leading to minimum. Easy to find valley, hard to converge. Global minimum $f(1,\ldots,1) = 0$. Domain: $[-5, 10]^n$.

\subsection{Parameter Configurations}

Tested three configurations (10D, 3 trials each):

\begin{itemize}
\item \textbf{Config 1 (Standard):} swarm=30, $w=0.7$, $c_1=1.5$, $c_2=1.5$, iter=1000
\item \textbf{Config 2 (Large Swarm):} swarm=50, $w=0.5$, $c_1=2.0$, $c_2=2.0$, iter=1000
\item \textbf{Config 3 (High Inertia):} swarm=40, $w=0.9$, $c_1=1.2$, $c_2=1.2$, iter=1500
\end{itemize}

\subsection{Results}

\begin{table}[h]
\centering
\caption{PSO Results on Rastrigin Function (10D)}
\label{tab:pso-rastrigin}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Configuration} & \textbf{Best} & \textbf{Avg} & \textbf{Time (s)} \\
\midrule
Config 1 (Standard) & 42.15 & 58.23 & 0.0165 \\
Config 2 (Large Swarm) & 35.87 & 51.42 & 0.0243 \\
Config 3 (High Inertia) & 38.92 & 53.67 & 0.0289 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{PSO Results on Rosenbrock Function (10D)}
\label{tab:pso-rosenbrock}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Configuration} & \textbf{Best} & \textbf{Avg} & \textbf{Time (s)} \\
\midrule
Config 1 (Standard) & 892.45 & 1203.67 & 0.0025 \\
Config 2 (Large Swarm) & 654.23 & 987.34 & 0.0041 \\
Config 3 (High Inertia) & 723.18 & 1089.52 & 0.0051 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Inertia weight (}$w$\textbf{):} Higher inertia (Config 3, $w=0.9$) encourages exploration by maintaining particle momentum. Lower inertia (Config 2, $w=0.5$) promotes exploitation by allowing stronger attraction to bests. For Rastrigin's many local minima, balanced inertia (Config 1, $w=0.7$) performs well. For Rosenbrock, lower inertia helps convergence in narrow valley (Config 2 best).

\textbf{Cognitive/Social (}$c_1, c_2$\textbf{):} Higher values (Config 2) increase attraction to personal/global bests, promoting faster convergence but risking premature convergence. Lower values (Config 3) allow more independent exploration. Balance (Config 1) works well generally.

\textbf{Swarm size:} Larger swarms (Config 2, 50 particles) provide better search space coverage, finding better solutions on both functions. Trade-off: higher computational cost per iteration.

\textbf{Function characteristics:} Rastrigin's multimodality requires balance between exploration (escape local minima) and exploitation (converge to good regions). Rosenbrock's narrow valley favors strong social component (Config 2) to navigate valley once found.

\section{Part C2: PSO for Sudoku}

\subsection{Formulation}

Sudoku as optimization problem:
\begin{itemize}
\item \textbf{Objective:} Minimize constraint violations
\item \textbf{Search space:} Complete 9×9 boards with given cells fixed
\item \textbf{Fitness:} Count duplicate values in columns and boxes
\end{itemize}

Each particle maintains row permutations (0 row violations), focusing optimization on column/box constraints.

\subsection{Discrete PSO Adaptation}

Standard PSO designed for continuous spaces. Adaptations for discrete Sudoku:

\textbf{Initialization:} Each particle initialized with valid row permutations (values 1-9 appear exactly once per row). Given cells locked. Remaining cells filled randomly.

\textbf{Velocity as swaps:} Instead of real-valued vectors, velocity represented as sequence of swap operations. Each swap exchanges two cells within same row.

\textbf{Probabilistic updates:}
\begin{itemize}
\item Inertia ($w$): Apply random exploration swaps
\item Cognitive ($c_1$): Probabilistically swap to match personal best
\item Social ($c_2$): Probabilistically swap to match global best
\end{itemize}

\subsection{Results}

\begin{table}[h]
\centering
\caption{PSO Results on Sudoku Puzzle (swarm=150, iter=3000)}
\label{tab:pso-sudoku}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Trial} & \textbf{Violations} & \textbf{Iterations} & \textbf{Time (s)} \\
\midrule
1 & 8 & 3000 & 7.25 \\
2 & 9 & 3000 & 7.56 \\
3 & 7 & 3000 & 7.42 \\
\midrule
\textbf{Average} & \textbf{8.0} & \textbf{3000} & \textbf{7.41} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{Performance:} PSO reduced violations from initial $\sim$18 to final $\sim$8 (55\% reduction) but did not fully solve puzzle (reach 0 violations). This demonstrates partial optimization capability.

\textbf{Comparison to CSP:} Unlike systematic CSP methods which guarantee finding solution if solvable, PSO provides no such guarantee. CSP methods (Part A) solve identical puzzle in <0.02s with certainty. PSO takes 7+ seconds and reaches only partial solution.

\textbf{Challenges for PSO:} Sudoku's discrete nature, complex constraint interactions, and requirement for exact satisfaction (not approximation) make it difficult for PSO. Swap-based velocity updates less natural than continuous arithmetic. Fixed cells constrain search space significantly.

\textbf{When metaheuristics useful:} PSO and similar metaheuristics better suited for problems where:
\begin{itemize}
\item Approximate solutions acceptable (vs exact satisfaction)
\item Continuous search spaces (vs discrete/combinatorial)
\item Systematic search intractable (vs Sudoku where CSP tractable)
\item Multiple competing objectives need balancing
\end{itemize}

For Sudoku specifically, CSP methods (Part A) clearly superior in both solution quality and efficiency.

\section{Conclusion}

This work demonstrates effectiveness of systematic search enhancements for CSPs and applicability (with limitations) of metaheuristics to combinatorial problems.

\textbf{Key Findings:}
\begin{itemize}
\item \textbf{Heuristics matter:} MRV+LCV provide 2.7-7.1× speedup over basic backtracking on Sudoku
\item \textbf{Constraint propagation powerful:} AC-3 achieves near-constant time regardless of puzzle difficulty
\item \textbf{Local search scales:} Minimum Conflicts solves n-Queens in empirical $O(n)$ time, far better than exponential backtracking
\item \textbf{Parameter tuning important:} PSO performance varies significantly with inertia, cognitive/social coefficients, and swarm size
\item \textbf{Right tool for job:} Systematic methods (CSP) excel on well-structured constraint problems; metaheuristics better for continuous optimization
\end{itemize}

\textbf{Implementation Quality:}
All algorithms implemented from scratch without specialized libraries, with extensive documentation, timeout protection, and comprehensive testing achieving 100\% test pass rate.

\textbf{Future Work:}
\begin{itemize}
\item Investigate other metaheuristics (Differential Evolution, Ant Colony Optimization) for comparison with PSO
\item Extend CSP methods to expert-level Sudoku puzzles with minimal clues
\item Scale Minimum Conflicts to $n > 100$ for n-Queens
\item Develop hybrid approaches combining CSP systematic search with metaheuristic guidance
\end{itemize}

\bibliographystyle{aaai24}
\bibliography{references}

\end{document}
